#!/usr/bin/env python3
"""
KingStore Fast Import - Optimized for millions of records
Uses batch inserts, prepared statements, and optimized queries
"""

import sys
from pathlib import Path
from datetime import datetime
import multiprocessing
from concurrent.futures import ProcessPoolExecutor, as_completed
import psycopg2
from psycopg2.extras import execute_batch, execute_values
import os
from collections import defaultdict

# Import parsing functions
from kingstore_simple_import import (
    get_db_connection, 
    parse_price_xml, 
    log as simple_log,
    extract_gz
)

def log(message, worker_id=None):
    """Thread-safe logging"""
    timestamp = datetime.now().strftime('%H:%M:%S')
    prefix = f"[{worker_id}]" if worker_id else ""
    print(f"[{timestamp}]{prefix} {message}", flush=True)

def batch_upsert_prices(conn, price_batch):
    """
    Fast batch upsert using PostgreSQL's INSERT ... ON CONFLICT
    Much faster than calling upsert_price for each record
    """
    if not price_batch:
        return 0
    
    cur = conn.cursor()
    
    try:
        # Use execute_values for bulk insert
        # This is much faster than individual inserts
        insert_query = """
            INSERT INTO prices (
                product_id, 
                supplier_id, 
                store_id, 
                price, 
                currency,
                is_available,
                first_scraped_at,
                last_scraped_at,
                scraped_at
            ) VALUES %s
            ON CONFLICT DO NOTHING
        """
        
        # Prepare data: (product_id, supplier_id, store_id, price, currency, is_available, NOW(), NOW(), NOW())
        now = datetime.now()
        values = [
            (p['product_id'], p['supplier_id'], p['store_id'], p['price'], 'ILS', True, now, now, now)
            for p in price_batch
        ]
        
        execute_values(cur, insert_query, values, template=None, page_size=1000)
        
        inserted = cur.rowcount
        
        # For existing prices, update last_scraped_at in batch
        # Find existing prices that need update
        if inserted < len(price_batch):
            # Get product_ids, supplier_ids, store_ids that weren't inserted
            existing_batch = price_batch[inserted:]
            
            # Update last_scraped_at for existing prices (same price within tolerance)
            update_query = """
                UPDATE prices
                SET last_scraped_at = NOW(),
                    scraped_at = NOW()
                WHERE (product_id, supplier_id, COALESCE(store_id, -1), price) = ANY(%s)
                  AND ABS(price - %s) <= 0.01
            """
            
            # This is a simplified version - for full upsert logic, we'd need a more complex query
            # For now, we'll just insert new prices and update existing ones separately
            
        conn.commit()
        return inserted
        
    except Exception as e:
        conn.rollback()
        raise e
    finally:
        cur.close()

def import_products_and_prices_fast(conn, metadata, items):
    """Fast import with batch operations"""
    cur = conn.cursor()
    stats = {'products': 0, 'prices': 0, 'skipped': 0}
    
    try:
        # Get IDs once
        cur.execute("SELECT id FROM verticals WHERE slug = 'supermarket'")
        vertical_id = cur.fetchone()[0]
        
        cur.execute("SELECT id FROM suppliers WHERE slug = 'kingstore'")
        supplier_id = cur.fetchone()[0]
        
        # Get chain
        cur.execute("SELECT id FROM chains WHERE slug = 'kingstore'")
        chain_result = cur.fetchone()
        if chain_result:
            chain_id = chain_result[0]
        else:
            cur.execute("""
                INSERT INTO chains (name, slug, name_he)
                VALUES ('KingStore', 'kingstore', '◊ß◊ô◊†◊í ◊°◊ò◊ï◊®')
                ON CONFLICT (slug) DO NOTHING
                RETURNING id
            """)
            result = cur.fetchone()
            if result:
                chain_id = result[0]
            else:
                cur.execute("SELECT id FROM chains WHERE slug = 'kingstore'")
                chain_id = cur.fetchone()[0]
        
        # Get store table ID
        store_table_id = None
        if metadata.get('store_id'):
            cur.execute("""
                SELECT id FROM stores 
                WHERE chain_id = %s AND store_id = %s
                LIMIT 1
            """, (chain_id, metadata.get('store_id')))
            store_result = cur.fetchone()
            if store_result:
                store_table_id = store_result[0]
        
        conn.commit()
        
        # Batch 1: Collect all product lookups first
        product_lookup = {}  # barcode -> product_id
        products_to_create = []
        
        for item in items:
            if not item.get('item_name') or not item.get('price'):
                stats['skipped'] += 1
                continue
            
            barcode = item.get('item_code')
            if barcode:
                product_lookup[barcode] = None  # Will be filled later
        
        # Batch lookup existing products by barcode
        if product_lookup:
            barcodes = list(product_lookup.keys())
            cur.execute("""
                SELECT id, ean, manufacturer_code 
                FROM products 
                WHERE ean = ANY(%s) OR manufacturer_code = ANY(%s)
            """, (barcodes, barcodes))
            
            for row in cur.fetchall():
                product_id, ean, mfr_code = row
                if ean in product_lookup:
                    product_lookup[ean] = product_id
                if mfr_code and mfr_code in product_lookup:
                    product_lookup[mfr_code] = product_id
        
        # Batch 2: Create new products in batch
        products_batch = []
        for item in items:
            if not item.get('item_name') or not item.get('price'):
                continue
            
            barcode = item.get('item_code')
            product_id = product_lookup.get(barcode) if barcode else None
            
            if not product_id:
                # Prepare for batch insert
                description = f"{item.get('manufacturer_name', '')} {item.get('manufacturer_desc', '')}".strip()
                attributes = {
                    'unit_qty': item.get('unit_qty'),
                    'quantity': item.get('quantity'),
                    'unit_of_measure': item.get('unit_of_measure'),
                    'is_weighted': item.get('is_weighted') == '1',
                    'qty_in_package': item.get('qty_in_package'),
                    'manufacture_country': item.get('manufacture_country'),
                    'item_type': item.get('item_type'),
                    'chain_id': metadata.get('chain_id'),
                    'store_id': metadata.get('store_id'),
                    'store_name': metadata.get('store_name'),
                }
                attributes = {k: v for k, v in attributes.items() if v is not None}
                
                products_batch.append((
                    item['item_name'][:500],
                    description[:1000],
                    vertical_id,
                    barcode,
                    barcode,
                    json.dumps(attributes, ensure_ascii=False) if attributes else '{}'
                ))
        
        # Batch insert products
        if products_batch:
            import json
            cur.execute("""
                INSERT INTO products (name, description, vertical_id, ean, manufacturer_code, attributes)
                VALUES %s
                ON CONFLICT DO NOTHING
                RETURNING id, ean, manufacturer_code
            """, execute_values(cur, """
                INSERT INTO products (name, description, vertical_id, ean, manufacturer_code, attributes)
                VALUES %s
                ON CONFLICT DO NOTHING
                RETURNING id, ean, manufacturer_code
            """, products_batch, template=None, page_size=500))
            
            # Update product_lookup with newly created products
            if returned:
                for row in returned:
                    product_id, ean, mfr_code = row
                    if ean:
                        product_lookup[ean] = product_id
                    if mfr_code:
                        product_lookup[mfr_code] = product_id
            
            stats['products'] = cur.rowcount
        
        # Batch 3: Prepare all prices for batch insert
        price_batch = []
        for item in items:
            if not item.get('item_name') or not item.get('price'):
                continue
            
            try:
                price_value = float(item['price'])
            except (TypeError, ValueError):
                stats['skipped'] += 1
                continue
            
            barcode = item.get('item_code')
            product_id = product_lookup.get(barcode) if barcode else None
            
            if not product_id:
                # Try to find by name as fallback
                cur.execute("""
                    SELECT id FROM products 
                    WHERE name = %s AND vertical_id = %s
                    LIMIT 1
                """, (item['item_name'][:500], vertical_id))
                result = cur.fetchone()
                if result:
                    product_id = result[0]
                else:
                    stats['skipped'] += 1
                    continue
            
            price_batch.append({
                'product_id': product_id,
                'supplier_id': supplier_id,
                'store_id': store_table_id,
                'price': price_value
            })
        
        # Batch insert prices
        if price_batch:
            inserted = batch_upsert_prices(conn, price_batch)
            stats['prices'] = inserted
        
        conn.commit()
        
    except Exception as e:
        log(f"[ERROR] Import failed: {e}")
        conn.rollback()
    finally:
        cur.close()
    
    return stats

def process_single_file_fast(file_info):
    """Process a single file - optimized version"""
    file_path, worker_id = file_info
    
    # Import here (needed for multiprocessing)
    from pathlib import Path
    from kingstore_fast_import import get_db_connection, parse_price_xml, import_products_and_prices_fast
    
    stats = {'products': 0, 'prices': 0, 'skipped': 0}
    
    try:
        log(f"Processing: {Path(file_path).name}", worker_id)
        
        conn = get_db_connection()
        
        try:
            metadata, items = parse_price_xml(Path(file_path))
            if not metadata:
                return (Path(file_path).name, False, stats, "Failed to parse XML")
            
            log(f"Found {len(items)} items (Store: {metadata.get('store_name', 'Unknown')})", worker_id)
            
            stats = import_products_and_prices_fast(conn, metadata, items)
            
            log(f"‚úÖ +{stats['products']} products, +{stats['prices']} prices, {stats['skipped']} skipped", worker_id)
            
            return (Path(file_path).name, True, stats, None)
            
        finally:
            conn.close()
            
    except Exception as e:
        error_msg = str(e)
        log(f"‚ùå Error: {error_msg}", worker_id)
        return (Path(file_path).name, False, stats, error_msg)

def main():
    """Main entry point with optimized parallel processing"""
    if len(sys.argv) > 1:
        data_dir = Path(sys.argv[1])
    else:
        data_dir = Path("/app/data/kingstore/downloads")
    
    # More workers for high-volume imports
    num_workers = int(os.getenv('KINGSTORE_WORKERS', min(multiprocessing.cpu_count() * 2, 16)))
    
    log("=" * 80)
    log("KingStore FAST Import (Optimized)")
    log("=" * 80)
    log(f"Directory: {data_dir}")
    log(f"Parallel workers: {num_workers}")
    
    if not data_dir.exists():
        log(f"ERROR Directory not found: {data_dir}")
        return
    
    xml_files = sorted(data_dir.glob("Price*.xml"))
    log(f"Found: {len(xml_files)} XML files")
    
    if not xml_files:
        log("ERROR No files found!")
        return
    
    file_list = [(str(f), i % num_workers) for i, f in enumerate(xml_files)]
    
    total_stats = {'files': 0, 'products': 0, 'prices': 0, 'skipped': 0, 'errors': 0}
    start_time = datetime.now()
    
    log(f"\nüöÄ Starting FAST parallel import with {num_workers} workers...")
    
    with ProcessPoolExecutor(max_workers=num_workers) as executor:
        future_to_file = {
            executor.submit(process_single_file_fast, file_info): file_info[0] 
            for file_info in file_list
        }
        
        completed = 0
        total = len(file_list)
        
        for future in as_completed(future_to_file):
            completed += 1
            filename = future_to_file[future]
            
            try:
                result_filename, success, stats, error = future.result()
                
                if success:
                    total_stats['files'] += 1
                    total_stats['products'] += stats['products']
                    total_stats['prices'] += stats['prices']
                    total_stats['skipped'] += stats['skipped']
                    if completed % 10 == 0 or completed == total:
                        log(f"[{completed}/{total}] ‚úÖ {result_filename}")
                else:
                    total_stats['errors'] += 1
                    log(f"[{completed}/{total}] ‚ùå {result_filename}: {error}")
                    
            except Exception as e:
                total_stats['errors'] += 1
                log(f"[{completed}/{total}] ‚ùå {filename}: {e}")
    
    end_time = datetime.now()
    duration = (end_time - start_time).total_seconds()
    
    log("\n" + "=" * 80)
    log("SUMMARY")
    log("=" * 80)
    log(f"Files processed:    {total_stats['files']}")
    log(f"Products created:   {total_stats['products']}")
    log(f"Prices imported:    {total_stats['prices']}")
    log(f"Items skipped:      {total_stats['skipped']}")
    log(f"Errors:             {total_stats['errors']}")
    log(f"Duration:           {duration:.1f} seconds ({duration/60:.1f} minutes)")
    if duration > 0:
        log(f"Speed:              {total_stats['prices']/duration:.1f} prices/second")
    log("=" * 80)
    
    if total_stats['errors'] == 0:
        log("‚úÖ All files processed successfully!")
    else:
        log(f"‚ö†Ô∏è  {total_stats['errors']} files had errors")

if __name__ == "__main__":
    main()

