# ğŸ¯ ×ª×›× ×™×ª ×™×™×©×•× - Gogobe Global Price Platform

## ×ª××¨×™×š: 21 ×“×¦××‘×¨ 2025

---

## ğŸ“‹ ×¡×“×¨ ×¢×“×™×¤×•×™×•×ª × ×›×•×Ÿ

### âŒ ×œ× × ×›×•×Ÿ:
```
1. Frontend ××“×”×™×
2. AI/LLM
3. Global infrastructure
4. ×•××–... ××•×œ×™ ×™×™×‘×•× × ×ª×•× ×™×?
```

### âœ… × ×›×•×Ÿ:
```
1. âœ… ×× ×’× ×•×Ÿ ×§×œ×™×˜×ª ××—×™×¨×™× ×¢×•×‘×“ (×¢× ×›×œ ×”×—×•×§×™×!)
2. âœ… Microservices ×œ×˜×™×¤×•×œ ×‘×œ×•×’×™×§×”
3. âœ… Cache + DB optimization
4. âœ… API ×‘×¡×™×¡×™
5. â³ Frontend ×¤×©×•×˜
6. â³ LLM/AI (×¨×§ ××—×¨×™ ×©×”×‘×¡×™×¡ ×¢×•×‘×“!)
7. â³ Global scale
```

---

## ğŸ—ï¸ Phase 1: ×× ×’× ×•×Ÿ ×§×œ×™×˜×ª ××—×™×¨×™× (×©×‘×•×¢×•×ª 1-4)

### Week 1: Import Service (Go) - ×”×‘×¡×™×¡

```go
// services/import-service/main.go
package main

/*
×ª×¤×§×™×“: ×§×¨×™××ª ×§×‘×¦×™× XML/GZ ×•×©×œ×™×—×ª events ×œ-Kafka

×—×•×§×™× ×©×¦×¨×™×š ×œ×˜×¤×œ ×‘×”×:
1. âœ… ×‘×“×™×§×ª ×§×™×•× ×¡× ×™×£ (×œ× ×œ×™×¦×•×¨ ×›×¤×™×œ×•×™×•×ª)
2. âœ… ×‘×“×™×§×ª ×§×™×•× ××•×¦×¨ (×œ×¤×™ ×‘×¨×§×•×“)
3. âœ… ×× ×™×¢×ª ×›×¤×™×œ×•×™×•×ª ×‘××—×™×¨×™× (upsert_price)
4. âœ… Geocoding (××¡×™× ×›×¨×•× ×™)
5. âœ… ×§×™×©×•×¨ ×œ××‘ ××•×¦×¨ (××¡×™× ×›×¨×•× ×™)
6. âœ… ×”××¨×ª ××˜×‘×¢×•×ª
*/

import (
    "encoding/xml"
    "github.com/confluentinc/confluent-kafka-go/kafka"
    "compress/gzip"
    "io"
    "os"
)

type Product struct {
    Name     string  `xml:"ItemNm"`
    Barcode  string  `xml:"ItemCode"`
    Price    float64 `xml:"ItemPrice"`
    StoreID  string  `xml:"StoreId"`
}

type PriceFile struct {
    XMLName  xml.Name  `xml:"Root"`
    Products []Product `xml:"Items>Item"`
    StoreID  string    `xml:"StoreId"`
    ChainID  string    `xml:"ChainId"`
}

func main() {
    // 1. Setup Kafka producer
    producer, err := kafka.NewProducer(&kafka.ConfigMap{
        "bootstrap.servers": "kafka:9092",
        "compression.type":  "snappy",
        "batch.size":        16384,
        "linger.ms":         10,
    })
    if err != nil {
        panic(err)
    }
    defer producer.Close()

    // 2. Parse XML file
    file, _ := os.Open("prices.xml.gz")
    defer file.Close()
    
    gzReader, _ := gzip.NewReader(file)
    defer gzReader.Close()
    
    var priceFile PriceFile
    decoder := xml.NewDecoder(gzReader)
    decoder.Decode(&priceFile)

    // 3. Send events to Kafka
    for _, product := range priceFile.Products {
        event := map[string]interface{}{
            "type":      "product.raw",
            "region":    "IL",
            "chain_id":  priceFile.ChainID,
            "store_id":  product.StoreID,
            "barcode":   product.Barcode,
            "name":      product.Name,
            "price":     product.Price,
            "currency":  "ILS",
            "timestamp": time.Now(),
        }
        
        jsonEvent, _ := json.Marshal(event)
        
        producer.Produce(&kafka.Message{
            TopicPartition: kafka.TopicPartition{
                Topic:     &[]string{"products.raw"}[0],
                Partition: kafka.PartitionAny,
            },
            Value: jsonEvent,
        }, nil)
    }
    
    producer.Flush(15000)
    log.Printf("âœ… Sent %d products to Kafka", len(priceFile.Products))
}
```

---

### Week 2: Product Processor (Go) - ×”×œ×•×’×™×§×” ×”××¨×›×–×™×ª

```go
// services/product-processor/main.go
package main

/*
×ª×¤×§×™×“: ×¢×™×‘×•×“ ××•×¦×¨×™× ×¢× ×›×œ ×”×—×•×§×™×

×—×•×§×™×:
1. âœ… ×‘×“×™×§×” ×‘-Redis Cache (99% hit rate)
2. âœ… ×‘×“×™×§×” ×‘-DB (×¨×§ ×× ×œ× ×‘-Cache)
3. âœ… ×™×¦×™×¨×ª ××•×¦×¨ ×—×“×© (×× ×œ× ×§×™×™×)
4. âœ… Batch insert ×œ××—×™×¨×™× (1000 ×‘×›×œ ×¤×¢×)
5. âœ… upsert_price (×× ×™×¢×ª ×›×¤×™×œ×•×™×•×ª)
*/

import (
    "github.com/go-redis/redis/v8"
    "github.com/confluentinc/confluent-kafka-go/kafka"
    "database/sql"
    _ "github.com/lib/pq"
)

var (
    redisClient *redis.Client
    db          *sql.DB
    priceQueue  []Price
)

type Price struct {
    ProductID int64
    StoreID   int64
    Price     float64
    Currency  string
}

func main() {
    // Setup
    redisClient = redis.NewClient(&redis.Options{
        Addr: "redis:6379",
    })
    
    db, _ = sql.Open("postgres", "postgres://...")
    
    // Kafka consumer
    consumer, _ := kafka.NewConsumer(&kafka.ConfigMap{
        "bootstrap.servers": "kafka:9092",
        "group.id":          "product-processor",
        "auto.offset.reset": "earliest",
    })
    
    consumer.Subscribe("products.raw", nil)
    
    for {
        msg, _ := consumer.ReadMessage(-1)
        
        var event map[string]interface{}
        json.Unmarshal(msg.Value, &event)
        
        processProduct(event)
    }
}

func processProduct(event map[string]interface{}) {
    barcode := event["barcode"].(string)
    
    // ×—×•×§ 1: ×‘×“×™×§×” ×‘-Redis Cache
    productID := getProductFromCache(barcode)
    
    if productID == 0 {
        // ×—×•×§ 2: ×‘×“×™×§×” ×‘-DB
        productID = getProductFromDB(barcode)
        
        if productID == 0 {
            // ×—×•×§ 3: ×™×¦×™×¨×ª ××•×¦×¨ ×—×“×©
            productID = createProduct(event)
        }
        
        // ×©××™×¨×” ×‘-Cache ×œ×¤×¢× ×”×‘××”
        cacheProduct(barcode, productID)
    }
    
    // ×—×•×§ 4: ×”×•×¡×¤×” ×œ-Batch Queue
    priceQueue = append(priceQueue, Price{
        ProductID: productID,
        StoreID:   getStoreID(event["store_id"].(string)),
        Price:     event["price"].(float64),
        Currency:  event["currency"].(string),
    })
    
    // ×—×•×§ 5: Batch Insert (×›×œ 1000 ××—×™×¨×™×)
    if len(priceQueue) >= 1000 {
        batchInsertPrices(priceQueue)
        priceQueue = []Price{}
    }
}

func getProductFromCache(barcode string) int64 {
    val, err := redisClient.Get(ctx, "product:ean:"+barcode).Int64()
    if err != nil {
        return 0
    }
    return val
}

func getProductFromDB(barcode string) int64 {
    var id int64
    db.QueryRow(`
        SELECT id FROM products 
        WHERE ean = $1 OR manufacturer_code = $1
        LIMIT 1
    `, barcode).Scan(&id)
    return id
}

func createProduct(event map[string]interface{}) int64 {
    var id int64
    db.QueryRow(`
        INSERT INTO products (name, ean, vertical_id)
        VALUES ($1, $2, 1)
        ON CONFLICT (ean) DO UPDATE SET name = EXCLUDED.name
        RETURNING id
    `, event["name"], event["barcode"]).Scan(&id)
    
    log.Printf("âœ… Created product: %s (ID: %d)", event["name"], id)
    return id
}

func cacheProduct(barcode string, productID int64) {
    redisClient.Set(ctx, "product:ean:"+barcode, productID, 24*time.Hour)
}

func batchInsertPrices(prices []Price) {
    // ×—×•×§ 6: upsert_price (×× ×™×¢×ª ×›×¤×™×œ×•×™×•×ª)
    stmt, _ := db.Prepare(`
        SELECT upsert_price($1, $2, $3, $4, $5, TRUE, 0.01)
    `)
    defer stmt.Close()
    
    for _, price := range prices {
        stmt.Exec(
            price.ProductID,
            1, // supplier_id
            price.StoreID,
            price.Price,
            price.Currency,
        )
    }
    
    log.Printf("âœ… Inserted %d prices", len(prices))
}
```

---

### Week 3: Store Processor (Go) - × ×™×”×•×œ ×¡× ×™×¤×™×

```go
// services/store-processor/main.go
package main

/*
×ª×¤×§×™×“: × ×™×”×•×œ ×¡× ×™×¤×™×

×—×•×§×™×:
1. âœ… ×‘×“×™×§×ª ×§×™×•× ×¡× ×™×£ (ON CONFLICT)
2. âœ… ×¢×“×›×•×Ÿ ×¤×¨×˜×™× (×× ×”×©×ª× ×•)
3. âœ… ×©×œ×™×—×ª event ×œ-Geocoding (××¡×™× ×›×¨×•× ×™)
*/

func processStore(event map[string]interface{}) {
    chainID := getChainID(event["chain_id"].(string))
    storeCode := buildStoreIdentifier(event)
    
    // ×—×•×§ 1+2: Get or Create + Update
    var storeID int64
    db.QueryRow(`
        INSERT INTO stores (
            chain_id, store_id, name, city, address, bikoret_no
        )
        VALUES ($1, $2, $3, $4, $5, $6)
        ON CONFLICT (chain_id, store_id) 
        DO UPDATE SET
            name = EXCLUDED.name,
            city = EXCLUDED.city,
            address = EXCLUDED.address,
            bikoret_no = EXCLUDED.bikoret_no
        RETURNING id
    `, chainID, storeCode, event["name"], event["city"], 
       event["address"], event["bikoret_no"]).Scan(&storeID)
    
    // ×—×•×§ 3: ×©×œ×™×—×ª event ×œ-Geocoding (×¨×§ ×× ××™×Ÿ GPS)
    if !hasGPS(storeID) {
        sendToGeocoding(storeID, event)
    }
    
    log.Printf("âœ… Store processed: %s (ID: %d)", event["name"], storeID)
}

func hasGPS(storeID int64) bool {
    var lat float64
    db.QueryRow(`
        SELECT latitude FROM stores WHERE id = $1
    `, storeID).Scan(&lat)
    return lat != 0
}

func sendToGeocoding(storeID int64, event map[string]interface{}) {
    geocodingEvent := map[string]interface{}{
        "type":      "geocoding.request",
        "store_id":  storeID,
        "address":   event["address"],
        "city":      event["city"],
    }
    
    // ×©×œ×™×—×” ×œ-Kafka
    producer.Produce(&kafka.Message{
        TopicPartition: kafka.TopicPartition{
            Topic: &[]string{"geocoding.requests"}[0],
        },
        Value: jsonEvent,
    }, nil)
}
```

---

### Week 4: Geocoding Service (Go) - GPS ××¡×™× ×›×¨×•× ×™

```go
// services/geocoding-service/main.go
package main

/*
×ª×¤×§×™×“: Geocoding ××¡×™× ×›×¨×•× ×™

×—×•×§×™×:
1. âœ… ×œ× ×—×•×¡× ××ª ×”×™×™×‘×•×
2. âœ… Rate limiting (1 req/sec)
3. âœ… Retry logic
4. âœ… Cache ×ª×•×¦××•×ª
*/

func main() {
    consumer, _ := kafka.NewConsumer(&kafka.ConfigMap{
        "bootstrap.servers": "kafka:9092",
        "group.id":          "geocoding-service",
    })
    
    consumer.Subscribe("geocoding.requests", nil)
    
    // Rate limiter - 1 request per second
    limiter := time.NewTicker(1 * time.Second)
    
    for {
        msg, _ := consumer.ReadMessage(-1)
        
        var event map[string]interface{}
        json.Unmarshal(msg.Value, &event)
        
        // ×”××ª×Ÿ ×œ-rate limiter
        <-limiter.C
        
        // Geocoding (async)
        go geocodeStore(event)
    }
}

func geocodeStore(event map[string]interface{}) {
    storeID := int64(event["store_id"].(float64))
    address := event["address"].(string)
    city := event["city"].(string)
    
    // ×—×•×§ 1: ×‘×“×™×§×” ×‘-Cache
    cacheKey := fmt.Sprintf("geo:%s:%s", address, city)
    cached := redisClient.Get(ctx, cacheKey).Val()
    
    var lat, lon float64
    
    if cached != "" {
        // Parse from cache
        fmt.Sscanf(cached, "%f,%f", &lat, &lon)
    } else {
        // ×—×•×§ 2: ×§×¨×™××” ×œ-OSM API
        lat, lon = callOSMAPI(address, city)
        
        // Cache ×œ×©× ×”
        redisClient.Set(ctx, cacheKey, 
            fmt.Sprintf("%f,%f", lat, lon), 
            365*24*time.Hour)
    }
    
    if lat != 0 && lon != 0 {
        // ×—×•×§ 3: ×¢×“×›×•×Ÿ DB
        db.Exec(`
            UPDATE stores 
            SET latitude = $1, longitude = $2,
                geom = ST_SetSRID(ST_MakePoint($2, $1), 4326)
            WHERE id = $3
        `, lat, lon, storeID)
        
        log.Printf("âœ… Geocoded store %d: %f, %f", storeID, lat, lon)
    }
}
```

---

## ğŸ“Š Phase 2: Optimization (×©×‘×•×¢×•×ª 5-6)

### Week 5: Redis Cache Layer

```go
// services/cache-service/cache.go
package cache

/*
××‘× ×” Cache:

1. Products by EAN:
   Key: "product:ean:7290000000001"
   Value: "12345" (product_id)
   TTL: 24 hours

2. Stores by ID:
   Key: "store:IL:001"
   Value: JSON {id, name, lat, lon}
   TTL: 24 hours

3. Chains:
   Key: "chain:name:Rami_Levy"
   Value: "153" (chain_id)
   TTL: 7 days

4. Exchange Rates:
   Key: "rate:ILS:USD"
   Value: "0.274"
   TTL: 1 hour
*/

func WarmupCache() {
    // ×˜×¢×™× ×” ×¨××©×•× ×™×ª ×©×œ × ×ª×•× ×™× × ×¤×•×¦×™×
    
    // 1. ×›×œ ×”×¨×©×ª×•×ª
    rows, _ := db.Query("SELECT id, name FROM store_chains")
    for rows.Next() {
        var id int64
        var name string
        rows.Scan(&id, &name)
        
        key := "chain:name:" + strings.ReplaceAll(name, " ", "_")
        redisClient.Set(ctx, key, id, 7*24*time.Hour)
    }
    
    // 2. ××•×¦×¨×™× ×¤×•×¤×•×œ×¨×™×™× (top 10K)
    rows, _ = db.Query(`
        SELECT id, ean FROM products 
        WHERE ean IS NOT NULL 
        ORDER BY id DESC 
        LIMIT 10000
    `)
    for rows.Next() {
        var id int64
        var ean string
        rows.Scan(&id, &ean)
        
        redisClient.Set(ctx, "product:ean:"+ean, id, 24*time.Hour)
    }
    
    log.Println("âœ… Cache warmed up")
}
```

---

### Week 6: Performance Testing

```bash
# test/load-test.sh

# 1. ×™×™×‘×•× 100,000 ××•×¦×¨×™×
echo "Testing import of 100K products..."
time docker exec import-service ./import-service --file=test-100k.xml.gz

# Expected: < 60 seconds

# 2. ×‘×“×™×§×ª Cache Hit Rate
echo "Checking Redis cache hit rate..."
docker exec redis-israel redis-cli INFO stats | grep keyspace_hits

# Expected: > 95%

# 3. ×‘×“×™×§×ª DB Load
echo "Checking PostgreSQL load..."
docker exec postgres-israel psql -U postgres -c "
    SELECT COUNT(*) FROM pg_stat_activity WHERE state = 'active';
"

# Expected: < 10 active connections

# 4. ×‘×“×™×§×ª Kafka Lag
echo "Checking Kafka consumer lag..."
docker exec kafka kafka-consumer-groups --bootstrap-server localhost:9092 \
    --describe --group product-processor

# Expected: lag < 100
```

---

## ğŸ¯ Success Criteria - Phase 1

### ×‘×™×¦×•×¢×™×:
- âœ… ×™×™×‘×•× 100K ××•×¦×¨×™× ×‘-< 60 ×©× ×™×•×ª
- âœ… Cache Hit Rate > 95%
- âœ… DB Queries < 1000/sec
- âœ… ××¤×¡ ×›×¤×™×œ×•×™×•×ª ×‘××—×™×¨×™×
- âœ… ××¤×¡ ×›×¤×™×œ×•×™×•×ª ×‘×¡× ×™×¤×™×

### ××™×›×•×ª × ×ª×•× ×™×:
- âœ… ×›×œ ××•×¦×¨ ×¢× ×‘×¨×§×•×“ ×ª×§×™×Ÿ
- âœ… ×›×œ ×¡× ×™×£ ×¢× GPS (×ª×•×š 24 ×©×¢×•×ª)
- âœ… ×›×œ ××—×™×¨ ×¢× timestamp
- âœ… ×”×™×¡×˜×•×¨×™×™×ª ××—×™×¨×™× (first_scraped_at, last_scraped_at)

### ×™×¦×™×‘×•×ª:
- âœ… ×× ×©×™×¨×•×ª × ×•×¤×œ - ×”×©××¨ ×××©×™×›×™×
- âœ… Kafka ××‘×˜×™×— delivery (at-least-once)
- âœ… Redis failover ××•×˜×•××˜×™
- âœ… DB backups ××•×˜×•××˜×™×™×

---

## ğŸ“ ××‘× ×” ×ª×™×§×™×•×ª

```
Gogobe/
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ import-service/          # Week 1
â”‚   â”‚   â”œâ”€â”€ main.go
â”‚   â”‚   â”œâ”€â”€ parser.go
â”‚   â”‚   â””â”€â”€ Dockerfile
â”‚   â”‚
â”‚   â”œâ”€â”€ product-processor/       # Week 2
â”‚   â”‚   â”œâ”€â”€ main.go
â”‚   â”‚   â”œâ”€â”€ cache.go
â”‚   â”‚   â”œâ”€â”€ batch.go
â”‚   â”‚   â””â”€â”€ Dockerfile
â”‚   â”‚
â”‚   â”œâ”€â”€ store-processor/         # Week 3
â”‚   â”‚   â”œâ”€â”€ main.go
â”‚   â”‚   â””â”€â”€ Dockerfile
â”‚   â”‚
â”‚   â””â”€â”€ geocoding-service/       # Week 4
â”‚       â”œâ”€â”€ main.go
â”‚       â”œâ”€â”€ osm.go
â”‚       â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ docker-compose.yml           # ×›×œ ×”×©×™×¨×•×ª×™×
â”œâ”€â”€ monitoring/
â”‚   â”œâ”€â”€ prometheus.yml
â”‚   â””â”€â”€ grafana-dashboards/
â”‚
â””â”€â”€ tests/
    â”œâ”€â”€ load-test.sh
    â””â”€â”€ integration-test.sh
```

---

## âœ… Next Steps

### ×¢×›×©×™×• (×©×‘×•×¢ 1):
```bash
# 1. ×¦×•×¨ Import Service
cd services/import-service
go mod init gogobe/import-service
go get github.com/confluentinc/confluent-kafka-go/kafka

# 2. ×›×ª×•×‘ ××ª ×”×§×•×“ (××”×“×•×’××” ×œ××¢×œ×”)
# 3. Build
docker build -t gogobe/import-service:v1 .

# 4. Test
docker run gogobe/import-service:v1 --file=test.xml.gz
```

### ×©×‘×•×¢ ×”×‘× (×©×‘×•×¢ 2):
- Product Processor
- Integration ×¢× Redis
- Batch insert logic

---

**×–×” ×”×‘×¡×™×¡ ×”× ×›×•×Ÿ - ×× ×’× ×•×Ÿ ×§×œ×™×˜×” ×¢×•×‘×“ ×¢× ×›×œ ×”×—×•×§×™×!** ğŸš€

**×¨×§ ××—×¨×™ ×©×–×” ×¢×•×‘×“ 100% - × ×¢×‘×•×¨ ×œ-LLM ×•-Global scale** âœ…
